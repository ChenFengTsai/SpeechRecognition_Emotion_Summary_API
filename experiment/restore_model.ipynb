{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13599cca-17b5-42ae-a061-4f35d1a06676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "label_to_num = {'admiration': 0,\n",
    " 'amusement': 1,\n",
    " 'anger': 2,\n",
    " 'annoyance': 3,\n",
    " 'approval': 4,\n",
    " 'caring': 5,\n",
    " 'confusion': 6,\n",
    " 'curiosity': 7,\n",
    " 'desire': 8,\n",
    " 'disappointment': 9,\n",
    " 'disapproval': 10,\n",
    " 'disgust': 11,\n",
    " 'embarrassment': 12,\n",
    " 'excitement': 13,\n",
    " 'fear': 14,\n",
    " 'gratitude': 15,\n",
    " 'grief': 16,\n",
    " 'joy': 17,\n",
    " 'love': 18,\n",
    " 'nervousness': 19,\n",
    " 'optimism': 20,\n",
    " 'pride': 21,\n",
    " 'realization': 22,\n",
    " 'relief': 23,\n",
    " 'remorse': 24,\n",
    " 'sadness': 25,\n",
    " 'surprise': 26,\n",
    " 'neutral': 27}\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2de76a2-c36c-4b15-b66b-365a97dac57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored successfully!\n"
     ]
    }
   ],
   "source": [
    "model_path = './checkpoints'\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(model_path)\n",
    "checkpoint.restore(latest_checkpoint)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "if status.assert_existing_objects_matched():\n",
    "    print(\"Model restored successfully!\")\n",
    "else:\n",
    "    print(\"Model not restored!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9fbf038-1e2e-40a9-b448-bde9f36392cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    './model_final',\n",
    "    custom_objects={'TFBertForSequenceClassification': TFBertForSequenceClassification}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0bce8c55-dc3e-4c9e-ba00-fb9cd4c4ede0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_final/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model_final/assets\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model_path = './checkpoints'\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "latest_checkpoint = tf.train.latest_checkpoint(model_path)\n",
    "checkpoint.restore(latest_checkpoint)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    './model_final',\n",
    "    save_format='tf',\n",
    "    include_optimizer=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cde82296-1e12-4855-928d-67fc529b20bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_341 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  21532     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,503,772\n",
      "Trainable params: 109,503,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "901f8960-38c7-4e67-8f9b-0b10eb3e9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(pred_sentences):\n",
    "    \n",
    "    tf_batch = tokenizer(pred_sentences, max_length=128, padding=True, truncation=True, return_tensors='tf')\n",
    "    tf_outputs = model(tf_batch)\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label= label.numpy()\n",
    "    key_list = list(label_to_num.keys())\n",
    "    val_list = list(label_to_num.values())\n",
    "\n",
    "    label = tf.argmax(tf_predictions, axis=1)\n",
    "    label = label.numpy()\n",
    "    for i in range(len(pred_sentences)):\n",
    "        position = val_list.index(label[i])\n",
    "        return key_list[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1294a0cf-f607-493a-9be4-01029c1df9eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admiration'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sentences = [\"What an excellent sequel - I, in fact, like it more than its predecessor.'Top Gun: Maverick' is fantastic, simply put. I was expecting it to be good, but it's actually much more enjoyable than I had anticipated. The callbacks to the original are expertly done, the new characters are strong/well cast, it has plenty of meaning, music is fab and the action is outstanding - the aerial stuff is sensational. The story is superb, with each high stake coming across as intended - parts even gave me slight goosebumps, which is a surprise given I'm not someone who has a connection to the 1986 film. It's all super neatly put together, I honestly came close to giving it a higher rating. Tom Cruise is brilliant as he reprises the role of Maverick, while Miles Teller comes in and gives a top performance. Jennifer Connelly is another positive, though her role does kinda feel a tiny bit forced in order to have a love interest; given Kelly McGillis' (unexplained) absence. Monica Barbaro stands out most from the fresh faces, though I actually did enjoy watching them all - which is something I thought the film may struggle with, adding new people, but it's done nicely; sure Jon Hamm and Glen Powell are a little clich√©, though overall I approve. A great watch - I'd highly recommend it, though naturally would suggest watching the previous film first if you haven't already.\"]\n",
    "prediction(pred_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ee493-7b31-4f95-be81-587796b40db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
